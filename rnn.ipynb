{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN classification\n",
    "\n",
    "Lets try and build an RNN to classify our disaster tweets.\n",
    "\n",
    "First we create the pipeline we build in 'data_inspection.ipynb'. \n",
    "Next we load the data from disk, transform the data and drop the columns we won't need in order to save some memory. Next, for clarity we define X and y.\n",
    "\n",
    "NOTE: set the PARALELL_JOBS constant, it is used as for the 'n_jobs' parameter where multi-processing is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer transforming data on 5 processes.\n",
      "tokenizer transforming data on 5 processes.\n",
      "urlRemover transforming data on 5 processes.\n",
      "urlRemover transforming data on 5 processes.\n",
      "punctuationRemover transforming data on 5 processes.\n",
      "punctuationRemover transforming data on 5 processes.\n",
      "numericsFilter transforming data on 5 processes.\n",
      "numericsFilter transforming data on 5 processes.\n",
      "stopwordsFilter transforming data on 5 processes.\n",
      "stopwordsFilter transforming data on 5 processes.\n",
      "SnakeCaseSplitter transforming data on 5 processes.\n",
      "SnakeCaseSplitter transforming data on 5 processes.\n",
      "Vectorizer transforming data.\n",
      "Vectorizer transforming data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:12<00:00, 625.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'keyword', 'location', 'text', 'target', 'tokens', 'vectors'], dtype='object')\n",
      "todo: remove excess columns\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_cleaning.transformers import tokenizer, urlRemover, punctuationRemover, SnakeCaseSplitter, numericsFilter, stopwordsFilter, Vectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "main_logger = logging.getLogger()\n",
    "main_logger.setLevel(logging.DEBUG)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "main_logger.addHandler(stdout_handler)\n",
    "\n",
    "MAX_TOKEN_LEN = 25\n",
    "VECTOR_DIM = 300\n",
    "PARALELL_JOBS = 5\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "                         \n",
    "pipeline = Pipeline([('tokenize', tokenizer(PARALELL_JOBS)), ('remove_urls', urlRemover(PARALELL_JOBS)), \n",
    "                     ('remove_punctuation', punctuationRemover(PARALELL_JOBS)), ('remove_numerics', numericsFilter(PARALELL_JOBS)), \n",
    "                     ('stopwords_filter', stopwordsFilter(PARALELL_JOBS)), ('snake_case_splitting', SnakeCaseSplitter(PARALELL_JOBS)),\n",
    "                     ('vectorize', Vectorizer(ft, MAX_TOKEN_LEN))])\n",
    "\n",
    "df = pd.read_csv('resources/data/train.csv')\n",
    "df = pipeline.transform(df)\n",
    "df.drop(['text', 'tokens', 'keyword', 'location'], 1)\n",
    "\n",
    "X = np.array(df['vectors'].to_list())\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNN\n",
    "\n",
    "Lets build a function to create a basic RNN using the Keras api. It uses a lstm with 100 units and is connected to a dense layer of 10 neurons and uses the adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def basic_rnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(MAX_TOKEN_LEN, VECTOR_DIM), dtype='float32'))\n",
    "    model.add(layers.LSTM(100))\n",
    "    model.add(layers.Dense(10))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need two more functions for showing the cross validation scores (both accuracy and f1), and displaying a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from statistics import variance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_x_val_scores(scores: dict):\n",
    "    mean_accuracy = round(sum(scores['test_accuracy']) / len(scores['test_accuracy']), 2)\n",
    "    accuracy_sqrt_var = round(sqrt(variance(scores['test_accuracy'])), 3)\n",
    "\n",
    "    mean_f1 = round(sum(scores['test_f1']) / len(scores['test_f1']), 2)\n",
    "    f1_sqrt_var = round(sqrt(variance(scores['test_f1'])), 3)\n",
    "\n",
    "    print('scores')\n",
    "    print(f\"accuracy: {mean_accuracy} average, {accuracy_sqrt_var} squared variance\")\n",
    "    print(f\"f1: {mean_f1} average, {f1_sqrt_var} squared variance\")\n",
    "    \n",
    "def conf_matrix(y_true, y_pred):\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred, [1, 0])\n",
    "    print(f'\\rconfusion matrix (n={len(y_true)})')\n",
    "    print('\\pred:  false | true ')\n",
    "    print('truth -----------------')\n",
    "    print(f'false:|  {TN} |  {FP}  |')\n",
    "    print('      |-------|-------|')\n",
    "    print(f'true :|  {FN}  |  {TP} |')\n",
    "    print('      -----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the model and wrap it in a KerasClassifier so we can use the keras cross validation function. We use the `show_x_val_scores` function to display the cross validation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "accuracy: 0.79 average, 0.019 squared variance\n",
      "f1: 0.73 average, 0.038 squared variance\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "    \n",
    "model = KerasClassifier(build_fn=basic_rnn)\n",
    "\n",
    "print('Running cross validation, this will take a minute.')\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=['accuracy', 'f1'], n_jobs=PARALELL_JOBS)\n",
    "show_x_val_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some more insight into the models behavior lets display a confusion matrix using the `conf_matrix` method we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples\n",
      "6090/6090 [==============================] - 16s 3ms/sample - loss: 0.5073 - accuracy: 0.7475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jos\\anaconda3\\envs\\disaster_tweets\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=[1, 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-167dd8c06c5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-76a4bacfe1d2>\u001b[0m in \u001b[0;36mconf_matrix\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mTN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'\\rconfusion matrix (n={len(y_true)})'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\pred:  false | true '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_tweets",
   "language": "python",
   "name": "disaster_tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
