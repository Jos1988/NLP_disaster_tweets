{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN classification\n",
    "\n",
    "Let try classification with a Convolutional Neural Network. Where RNN are good add evaluating input in the context of previous input. A CNN is able to interate over multiple subsections of the data and evaluate these. When applying a CNN to text, it will focus on the different combinations of naburing words found in the text (n-grams). \n",
    "A famous paper on the subject was written by Yoon Kim (https://github.com/yoonkim/CNN_sentence).\n",
    "\n",
    "Lets try and build a simple CNN capable of performing predicions on our tweets.\n",
    "\n",
    "First we start with the basics and helper functions, please see the RNN notebook for more information. \n",
    "And lets setup the data pipeline, please refer to the data_analysis notebook for more information aswell.\n",
    "\n",
    "NOTE: The PARALELL_JOBS constant it is used as for the 'n_jobs' parameter where multi-processing is possible. Set it to a higher valued depending on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Fasttext model.\n",
      "loading dataset.\n",
      "tokenizer transforming data on 5 processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urlRemover transforming data on 5 processes.\n",
      "punctuationRemover transforming data on 5 processes.\n",
      "numericsFilter transforming data on 5 processes.\n",
      "stopwordsFilter transforming data on 5 processes.\n",
      "SnakeCaseSplitter transforming data on 5 processes.\n",
      "Vectorizer transforming data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jos\\anaconda3\\envs\\disaster_tweets\\lib\\site-packages\\tqdm\\std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:13<00:00, 545.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_cleaning.transformers import tokenizer, urlRemover, punctuationRemover, SnakeCaseSplitter, numericsFilter, stopwordsFilter, Vectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Setup logger.\n",
    "main_logger = logging.getLogger()\n",
    "main_logger.setLevel(logging.DEBUG)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "main_logger.addHandler(stdout_handler)\n",
    "\n",
    "# Parameters\n",
    "MAX_TOKEN_LEN = 25\n",
    "VECTOR_DIM = 300\n",
    "# Change this parameter to use multiprocessing where possible.\n",
    "PARALELL_JOBS = 5\n",
    "\n",
    "# Load fasttext embedding.\n",
    "print('Loading Fasttext model.')\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "# Create data pipeline.\n",
    "pipeline = Pipeline([('tokenize', tokenizer(PARALELL_JOBS)), ('remove_urls', urlRemover(PARALELL_JOBS)), \n",
    "                     ('remove_punctuation', punctuationRemover(PARALELL_JOBS)), ('remove_numerics', numericsFilter(PARALELL_JOBS)), \n",
    "                     ('stopwords_filter', stopwordsFilter(PARALELL_JOBS)), ('snake_case_splitting', SnakeCaseSplitter(PARALELL_JOBS)),\n",
    "                     ('vectorize', Vectorizer(ft, MAX_TOKEN_LEN))])\n",
    "\n",
    "# Load dataset.\n",
    "print('loading dataset.')\n",
    "df = pd.read_csv('resources/data/train.csv')\n",
    "df = pipeline.transform(df)\n",
    "df.drop(['text', 'tokens', 'keyword', 'location'], 1)\n",
    "\n",
    "# Balancing positive and negative classified tweets.\n",
    "y = df['target']\n",
    "false_count, true_count = y.value_counts()\n",
    "surplus_false_tweets = false_count - true_count\n",
    "false_indices = y[y == 0].index.to_list()\n",
    "indices_to_remove = false_indices[:surplus_false_tweets]\n",
    "\n",
    "X = np.array(df['vectors'].drop(indices_to_remove).to_list())\n",
    "y = np.array(df['target'].drop(indices_to_remove).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def conf_matrix(y_true, y_pred):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    TP = int(matrix[1][1])\n",
    "    FP = int(matrix[0][1])\n",
    "    TN = int(matrix[0][0])\n",
    "    FN = int(matrix[1][0])\n",
    "\n",
    "    print(f'\\rconfusion matrix (n={len(y_true)})')\n",
    "    print('\\pred:  false | true ')\n",
    "    print('truth -------------')\n",
    "    print(f'false:| {TN} | {FP} |')\n",
    "    print('      |-----|-----|')\n",
    "    print(f'true :| {FN} | {TP} |')\n",
    "    print('      -------------')\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = ((TP + TN) / (FN + FP + TN + TP)) * 100\n",
    "\n",
    "    print(f'\\rprecision: \\t{round(precision, 2)}, out of all your positives this part whas true.')\n",
    "    print(f'recall: \\t{round(recall, 2)}, out of all the positives, this is the part we caught.')\n",
    "    print(f'accuracy: \\t{round(accuracy, 2)}.')\n",
    "    print(f'f1: \\t\\t{round(2*((precision*recall)/(precision+recall)), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "Lets create a function for building our model. It will use filters ranging fom 1 to 5 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def yoon_cnn():\n",
    "    sequence_input = layers.Input(shape=(25, 300), dtype='float32')\n",
    "\n",
    "    filter_sizes = [1, 2, 3, 4, 5]\n",
    "\n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = layers.Conv1D(filters=200,\n",
    "                        kernel_size=filter_size,\n",
    "                        activation='relu')(sequence_input)\n",
    "        l_pool = layers.GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = layers.concatenate(convs, axis=1)\n",
    "    x = layers.Dense(10, activation='relu')(l_merge)\n",
    "    preds = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Now lets cross-validate the model. Because the `cross_validate` function is only compatible with Keras's `Sequential` model we'll have to write some custom logic.\n",
    "\n",
    "*Note where are importieng the worker function from a seperate file so the multiprocessing will work on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:\n",
      "accuracy: 0.78 average, 0.014 squared variance\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from sklearn.model_selection import KFold\n",
    "from workers import cnn_worker\n",
    "from statistics import variance\n",
    "from math import sqrt\n",
    "\n",
    "kf = KFold(n_splits=7, shuffle=True, random_state=1)\n",
    "folds = [(train_index, test_index, X, y) for train_index, test_index in kf.split(X)]\n",
    "\n",
    "pool = mp.Pool(PARALELL_JOBS)\n",
    "results = pool.map(cnn_worker, folds)\n",
    "pool.close()\n",
    "\n",
    "accuracies = [float(result[1]) for result in results]\n",
    "mean_accuracy = round(sum(accuracies) / len(accuracies), 2)\n",
    "accuracy_sqrt_var = round(sqrt(variance(accuracies)), 3)\n",
    "\n",
    "print('scores:')\n",
    "print(f\"accuracy: {mean_accuracy} average, {accuracy_sqrt_var} squared variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the same compared to the RNN trained in the RNN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5233 samples\n",
      "5233/5233 [==============================] - 12s 2ms/sample - loss: 0.5386 - acc: 0.7399\n",
      "confusion matrix (n=1309)\n",
      "\\pred:  false | true \n",
      "truth -------------\n",
      "false:| 546 | 103 |\n",
      "      |-----|-----|\n",
      "true :| 169 | 491 |\n",
      "      -------------\n",
      "precision: \t0.83, out of all your positives this part whas true.\n",
      "recall: \t0.74, out of all the positives, this is the part we caught.\n",
      "accuracy: \t79.22.\n",
      "f1: \t\t0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "model = yoon_cnn()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix(y_test, np.round(y_pred.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again The same performance as the RNN Model. This suggests that performance might be bottlenecked by something else other thatn the models architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_tweets",
   "language": "python",
   "name": "disaster_tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
