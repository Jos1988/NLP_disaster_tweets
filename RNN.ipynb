{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN classification\n",
    "\n",
    "Lets try and build an RNN to classify our disaster tweets.\n",
    "\n",
    "First we create the pipeline we build in 'data_inspection.ipynb'. \n",
    "Next we load the data from disk, transform the data and drop the columns we won't need in order to save some memory. Next, for clarity we define X and y.\n",
    "\n",
    "NOTE: The PARALELL_JOBS constant it is used as for the 'n_jobs' parameter where multi-processing is possible. Set it to a higher valued depending on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer transforming data on 5 processes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urlRemover transforming data on 5 processes.\n",
      "punctuationRemover transforming data on 5 processes.\n",
      "numericsFilter transforming data on 5 processes.\n",
      "stopwordsFilter transforming data on 5 processes.\n",
      "SnakeCaseSplitter transforming data on 5 processes.\n",
      "Vectorizer transforming data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jos\\anaconda3\\envs\\disaster_tweets\\lib\\site-packages\\tqdm\\std.py:670: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:11<00:00, 657.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_cleaning.transformers import tokenizer, urlRemover, punctuationRemover, SnakeCaseSplitter, numericsFilter, stopwordsFilter, Vectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "main_logger = logging.getLogger()\n",
    "main_logger.setLevel(logging.DEBUG)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "main_logger.addHandler(stdout_handler)\n",
    "\n",
    "MAX_TOKEN_LEN = 25\n",
    "VECTOR_DIM = 300\n",
    "# Change this parameter to use multiprocessing where possible.\n",
    "PARALELL_JOBS = 1\n",
    "\n",
    "fasttext.util.download_model('en', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "                         \n",
    "pipeline = Pipeline([('tokenize', tokenizer(PARALELL_JOBS)), ('remove_urls', urlRemover(PARALELL_JOBS)), \n",
    "                     ('remove_punctuation', punctuationRemover(PARALELL_JOBS)), ('remove_numerics', numericsFilter(PARALELL_JOBS)), \n",
    "                     ('stopwords_filter', stopwordsFilter(PARALELL_JOBS)), ('snake_case_splitting', SnakeCaseSplitter(PARALELL_JOBS)),\n",
    "                     ('vectorize', Vectorizer(ft, MAX_TOKEN_LEN))])\n",
    "\n",
    "df = pd.read_csv('resources/data/train.csv')\n",
    "df = pipeline.transform(df)\n",
    "df.drop(['text', 'tokens', 'keyword', 'location'], 1)\n",
    "\n",
    "X = np.array(df['vectors'].to_list())\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RNN\n",
    "\n",
    "Now we have the data prepared lets start with a basic RNN. RNN's have been effective tools for interpreting text as they are capable of interpreting sequences of data. Meaning that the when the model is 'reading' a tweet it will have some notion of the previous words while interpreting the next word.\n",
    "\n",
    "We wil using the Keras api to build the model, Keras is easy to read and allows us to build a fully functioning RNN in just a few lines of code. The model will be a LSTM with 100 units connected to a dense layer of 10 neurons and using an adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def basic_rnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(MAX_TOKEN_LEN, VECTOR_DIM), dtype='float32'))\n",
    "    model.add(layers.LSTM(100))\n",
    "    model.add(layers.Dense(10))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define two more functions. One for showing the cross validation scores, and another for  displaying a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from statistics import variance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def show_x_val_scores(scores: dict):\n",
    "    mean_accuracy = round(sum(scores['test_accuracy']) / len(scores['test_accuracy']), 2)\n",
    "    accuracy_sqrt_var = round(sqrt(variance(scores['test_accuracy'])), 3)\n",
    "\n",
    "    mean_f1 = round(sum(scores['test_f1']) / len(scores['test_f1']), 2)\n",
    "    f1_sqrt_var = round(sqrt(variance(scores['test_f1'])), 3)\n",
    "\n",
    "    print('scores')\n",
    "    print(f\"accuracy: {mean_accuracy} average, {accuracy_sqrt_var} squared variance\")\n",
    "    print(f\"f1: {mean_f1} average, {f1_sqrt_var} squared variance\")\n",
    "    \n",
    "def conf_matrix(y_true, y_pred):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    TP = int(matrix[1][1])\n",
    "    FP = int(matrix[0][1])\n",
    "    TN = int(matrix[0][0])\n",
    "    FN = int(matrix[1][0])\n",
    "    \n",
    "    print(f'\\rconfusion matrix (n={len(y_true)})')\n",
    "    print('\\pred:  false | true ')\n",
    "    print('truth -------------')\n",
    "    print(f'false:| {TN} | {FP} |')\n",
    "    print('      |-----|-----|')\n",
    "    print(f'true :| {FN} | {TP} |')\n",
    "    print('      -------------')\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    accuracy = ((TP + TN) / (FN + FP + TN + TP)) * 100\n",
    "    \n",
    "    print(f'\\rprecision: \\t{round(precision, 2)}, out of all your positives this part whas true.')\n",
    "    print(f'recall: \\t{round(recall, 2)}, out of all the positives, this is the part we caught.')\n",
    "    print(f'accuracy: \\t{round(accuracy, 2)}.')\n",
    "    print(f'f1: \\t\\t{round(2*((precision*recall)/(precision+recall)), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build the model and wrap it in a KerasClassifier so we can use the sklearn cross_validation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross validation, this will take a minute.\n",
      "scores\n",
      "accuracy: 0.78 average, 0.018 squared variance\n",
      "f1: 0.72 average, 0.04 squared variance\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "    \n",
    "model = KerasClassifier(build_fn=basic_rnn)\n",
    "\n",
    "print('Running cross validation, this will take a minute.')\n",
    "scores = cross_validate(model, X, y, cv=5, scoring=['accuracy', 'f1'], n_jobs=PARALELL_JOBS)\n",
    "show_x_val_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validating the model across 5 splits results in a 78% accuracy, so 78% of all predictions are correct. The F1 score is a bit lower.\n",
    "\n",
    "To get some more insight into the models performance lets display a confusion matrix using the `conf_matrix` method we wrote earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples\n",
      "6090/6090 [==============================] - 21s 3ms/sample - loss: 0.5057 - accuracy: 0.7614\n",
      "confusion matrix (n=1523)\n",
      "\\pred:  false | true \n",
      "truth -------------\n",
      "false:| 763 | 119 |\n",
      "      |-----|-----|\n",
      "true :| 188 | 453 |\n",
      "      -------------\n",
      "precision: \t0.79, out of all your positives this part whas true.\n",
      "recall: \t0.71, out of all the positives, this is the part we caught.\n",
      "accuracy: \t79.84.\n",
      "f1: \t\t0.75\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "conf_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to have more false negatives (bottom left of the matrix) than false positives (top right of the matrix). This means that the model is slightly biased towards classifying tweets as a disaster tweet.\n",
    "\n",
    "This explains the lower f1 score, it is possibly the result of the imbalance between the number of positively(3271) classified tweets and the number of negatively classified tweets(4342).\n",
    "\n",
    "If we balance the dataset we might get different scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5233 samples\n",
      "5233/5233 [==============================] - 20s 4ms/sample - loss: 0.5198 - accuracy: 0.7430\n",
      "confusion matrix (n=1309)\n",
      "\\pred:  false | true \n",
      "truth -------------\n",
      "false:| 582 | 67 |\n",
      "      |-----|-----|\n",
      "true :| 199 | 461 |\n",
      "      -------------\n",
      "precision: \t0.87, out of all your positives this part whas true.\n",
      "recall: \t0.7, out of all the positives, this is the part we caught.\n",
      "accuracy: \t79.68.\n",
      "f1: \t\t0.78\n"
     ]
    }
   ],
   "source": [
    "# There are too many 'false' tweets, lets calculate how many\n",
    "false_count, true_count = y.value_counts()\n",
    "surplus_false_tweets = false_count - true_count\n",
    "\n",
    "# Get a list of indices to drop, in order to balance the data.\n",
    "false_indices = y[y == 0].index.to_list()\n",
    "indices_to_remove = false_indices[:surplus_false_tweets]\n",
    "\n",
    "# Create balanced dataset.\n",
    "X_bal = np.array(df['vectors'].drop(indices_to_remove).to_list())\n",
    "y_bal = df['target'].drop(indices_to_remove)\n",
    "\n",
    "# Repeat the process.\n",
    "X_bal_train, X_bal_test, y_bal_train, y_bal_test = train_test_split(X_bal, y_bal, test_size=0.2, random_state=1)\n",
    "model.fit(X_bal_train, y_bal_train)\n",
    "y_bal_pred = model.predict(X_bal_test)\n",
    "conf_matrix(y_bal_test, y_bal_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By balancing the dataset the f1 improved. So the imbalance whas a factor.\n",
    "\n",
    "## A better RNN?\n",
    "\n",
    "Now lets see if we can improve tha algorithm a bit. Maybe we can get a better score by changing the number of neurons, or using a GRU instead of a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(rnn_units: int, rnn_type: str, dense_units: int, bidirectional: bool):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(25, 300), dtype='float32'))\n",
    "\n",
    "    if rnn_type == 'lstm':\n",
    "        rnn = layers.LSTM(rnn_units)\n",
    "    elif rnn_type == 'gru':\n",
    "        rnn = layers.GRU(rnn_units)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid rnn type: {rnn_type}.\")\n",
    "\n",
    "    if bidirectional is True:\n",
    "        rnn = layers.Bidirectional(rnn)\n",
    "\n",
    "    model.add(rnn)\n",
    "    model.add(layers.Dense(dense_units))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above allows us to pass some hyperparameters to the model and do some fine tuning. Lets determint the hyperparametes we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'rnn_units': [100, 300], \n",
    "    'rnn_type': ['lstm', 'gru'], \n",
    "    'dense_units': [10, 100], \n",
    "    'bidirectional': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gridsearch object provided in the SKlearn library will allow us to try all combinations of the parameters in the grid above, while using crossvalidating the results. This wil be a total of 2 x 2 x 2 x 2 = 16 combinations. Crossvalidating with a cv of 5. This means that the Gridsearch object will be training 16 x 5 = 80 models in the next code block. I you want to reduce this you can lower the cv to 3 or 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best found model scored: 0.77.\n",
      "Using the following hyperparams{'bidirectional': True, 'dense_units': 10, 'rnn_type': 'gru', 'rnn_units': 100}.\n",
      "bidirectional: True\n",
      "dense_units: 10\n",
      "rnn_type: gru\n",
      "rnn_units: 100\n",
      "\n",
      "other models scored\n",
      "score: 0.77\n",
      "bidirectional: True\n",
      "dense_units: 10\n",
      "rnn_type: gru\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.77\n",
      "bidirectional: True\n",
      "dense_units: 10\n",
      "rnn_type: lstm\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.768\n",
      "bidirectional: True\n",
      "dense_units: 100\n",
      "rnn_type: gru\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.768\n",
      "bidirectional: True\n",
      "dense_units: 100\n",
      "rnn_type: gru\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.767\n",
      "bidirectional: True\n",
      "dense_units: 100\n",
      "rnn_type: lstm\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.767\n",
      "bidirectional: False\n",
      "dense_units: 100\n",
      "rnn_type: lstm\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.764\n",
      "bidirectional: True\n",
      "dense_units: 10\n",
      "rnn_type: gru\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.761\n",
      "bidirectional: False\n",
      "dense_units: 10\n",
      "rnn_type: lstm\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.76\n",
      "bidirectional: False\n",
      "dense_units: 100\n",
      "rnn_type: lstm\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.76\n",
      "bidirectional: False\n",
      "dense_units: 100\n",
      "rnn_type: gru\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.758\n",
      "bidirectional: True\n",
      "dense_units: 10\n",
      "rnn_type: lstm\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.757\n",
      "bidirectional: False\n",
      "dense_units: 10\n",
      "rnn_type: lstm\n",
      "rnn_units: 100\n",
      "\n",
      "score: 0.756\n",
      "bidirectional: True\n",
      "dense_units: 100\n",
      "rnn_type: lstm\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.755\n",
      "bidirectional: False\n",
      "dense_units: 100\n",
      "rnn_type: gru\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.755\n",
      "bidirectional: False\n",
      "dense_units: 10\n",
      "rnn_type: gru\n",
      "rnn_units: 300\n",
      "\n",
      "score: 0.751\n",
      "bidirectional: False\n",
      "dense_units: 10\n",
      "rnn_type: gru\n",
      "rnn_units: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = KerasClassifier(build_fn=rnn)\n",
    "gs = GridSearchCV(estimator=model, param_grid=parameters, cv=5, n_jobs=PARALELL_JOBS, scoring='accuracy', verbose=2)\n",
    "gs.fit(X_bal, y_bal)\n",
    "\n",
    "print(f'The best found model scored: {round(gs.best_score_,3)}.')\n",
    "print(f'Using the following hyperparams{gs.best_params_}.')\n",
    "for param, value in gs.best_params_.items():\n",
    "    print(f'{param}: {value}')\n",
    "print()\n",
    "\n",
    "result = [(params, mean_score) for params, mean_score in zip(gs.cv_results_['params'], gs.cv_results_['mean_test_score'])]\n",
    "def sort_fn(x):\n",
    "    return x[1]\n",
    "\n",
    "result.sort(reverse=True, key=sort_fn)\n",
    "\n",
    "print('other models scored')\n",
    "for params, mean_score in result:\n",
    "    print(f'score: {round(mean_score, 3)}')\n",
    "    for param, value in params.items():\n",
    "        print(f'{param}: {value}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disaster_tweets",
   "language": "python",
   "name": "disaster_tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
